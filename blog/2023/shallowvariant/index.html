<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>ShallowVariant | Paul  Grosu</title>
    <meta name="author" content="Paul  Grosu">
    <meta name="description" content="This is a blog to illustrate the ideas of a neural network through a simplified variant caller named ShallowVariant.">
    <meta name="keywords" content="Paul Grosu, Genomics, Machine Learning, Artificial Intelligence">


    <!-- Bootstrap & MDB -->
    <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
    <!-- <link rel="stylesheet" href="/assets/css/mdb.min.css?62a43d1430ddb46fc4886f9d0e3b49b8"> -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/paul_grosu_photo.jpg?bf5f93dfc1b8b6489cf0316b74964d68">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://pgrosu.github.io/blog/2023/shallowvariant/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?3dd82e91913a2c1265c0f80e41ff39e2"></script>
    <script src="/assets/js/dark_mode.js?6458e63976eae16c0cbe86b97023895a"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpubnoa/template.v2.js"></script>
    <script src="/assets/js/distillpubnoa/transforms.v2.js"></script>
    <script src="/assets/js/distillpubnoa/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "ShallowVariant",
      "description": "This is a blog to illustrate the ideas of a neural network through a simplified variant caller named ShallowVariant.",
      "published": "November 4, 2023",
      "authors": [
        {
          "author": "Paul Grosu",
          "authorURL": "https://pgrosu.github.io/",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Paul </span>Grosu</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>ShallowVariant</h1>
        <p>This is a blog to illustrate the ideas of a neural network through a simplified variant caller named ShallowVariant.</p>
      </d-title><d-byline></d-byline><d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#downloading-the-data">Downloading the Data</a></div>
            <div><a href="#performing-the-pileup-to-generate-candidate-variants-with-truth-values">Performing the Pileup to Generate Candidate Variants with Truth Values</a></div>
            <div><a href="#defining-the-input-tensor-and-output-labels">Defining the Input (Tensor) and Output Labels</a></div>
            <div><a href="#loading-and-processing-the-data">Loading and Processing the Data</a></div>
            <div><a href="#defining-the-multiclass-neural-network-model">Defining the MultiClass Neural Network Model</a></div>
            <div><a href="#implementing-the-multiclass-neural-network-model">Implementing the MultiClass Neural Network Model</a></div>
            <div><a href="#training-the-model">Training the Model</a></div>
            <div><a href="#testing-the-model">Testing the Model</a></div>
            <div><a href="#making-some-predictions">Making Some Predictions</a></div>
            <div><a href="#what-has-the-model-learned-diving-deeper-into-the-genotype-model">What has the model learned? Diving Deeper Into the Genotype Model</a></div>
            
          </nav>
        </d-contents>

        <p>In the previous <a href="../wdml-part-1">blog</a> we discussed how a neural network works.  Here we will build a simpler variation of <a href="https://github.com/google/deepvariant" rel="external nofollow noopener" target="_blank">Google’s DeepVariant</a><d-cite key="poplin2018deepvariant"></d-cite> using a neural network, to illustrate a practical example in the area of Genomics with Machine Learning.</p>

<h2 id="downloading-the-data">Downloading the Data</h2>

<p>The first step is to download the necessary files:</p>

<ol>
  <li>
    <p>The first file we will download are the alignment files (BAM and BAI) for chromosome 20 of sample NA12878 (phase 3) from the 1000 Genomes dataset.<d-footnote>Sample NA12878 (1000 Genomes phase 3): <a href="https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/NA12878/alignment/" rel="external nofollow noopener" target="_blank">https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/NA12878/alignment/</a></d-footnote></p>
  </li>
  <li>
    <p>The next file we will download is the Human Genome assembly GRCh37 (specifically hs37d5), since that was the reference used for aligning the FASTQ sequences in generating the BAM file.<d-footnote>Human Genome assembly GRCh37 (hs37d5) reference: <a href="https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/" rel="external nofollow noopener" target="_blank">https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/</a></d-footnote></p>
  </li>
</ol>

<p>These two files are sufficient to get us started.  Throughout this blog other commonly used bioinformatic tools will be utilized to keep the example simple.</p>

<h2 id="performing-the-pileup-to-generate-candidate-variants-with-truth-values">Performing the Pileup to Generate Candidate Variants with Truth Values</h2>

<p>The next step is to extract candidate variants from the BAM file, by performing a pileup. A pileup is an aligment (mapping) of reads to a reference.  If there is a region of variation among the reads and reference, then these will become candidates.  In our case we will use <a href="https://github.com/freebayes/freebayes" rel="external nofollow noopener" target="_blank">freebayes</a>, but other tools like <a href="https://samtools.github.io/bcftools/howtos/variant-calling.html" rel="external nofollow noopener" target="_blank">bcftools mpileup</a> will also work.  The idea is to focus on bi-allelic SNPs (just to keep the problem simple), which we can use to build a neural network to predict the genotype.  Since a pileup provides the position, we would not need the bases, and can rely only on the <em>reference</em> and <em>alternate allelic depths</em>.  These provide information regarding the number of reads that support the reference or alternate allele.</p>

<p>Thus our goal is to see if we can estimate the genotypes from just the <em>reference</em> and <em>alternate allelic depths</em>, and use the truth values<d-footnote>Truth values denote what would be the known and expected empirically derived values.</d-footnote> of known genotypes to train the weights of our neural network appropriately $\text{–}$ with the hope of uncovering this correspondence between <em><ins>coverage</ins></em> (depths) and <em><ins>genotype</ins></em>.  Our hypothesis is that there is correlation between the coverage (depths) and genotype.  Thus, all we will need are the following elements:</p>

<ul>
  <li>Reference Counts</li>
  <li>Alternate Counts</li>
  <li>Genotype</li>
</ul>

<p>Therefore, we will run the following two commands to generate the answers:</p>

<d-code block="" language="bash">
# This command will generate the SNP candidates (among other ones), along with 
# their corresponding genotypes and allelic depths.  These will be the truth set.
freebayes -f hs37d5.fa \
  NA12878.chrom20.ILLUMINA.bwa.CEU.low_coverage.20121211.bam &gt; candidates-freebayes.vcf

# This command will extract the reference base, alternate base, genotype and 
# allelic depths.  These will be used in building the neural network.
bcftools query -f '%REF,%ALT,[%GT,%AD]' candidates-freebayes.vcf &gt; ref_alt_gt_ad.txt
</d-code>

<p>The output of <code class="language-plaintext highlighter-rouge">ref_alt_gt_ad.txt</code> will look something like this (for just the bi-allelic SNPs):</p>

<d-code block="" language="bash">
A,C,1/1,0,5
G,A,1/1,0,5
G,C,1/1,0,5
T,C,1/1,0,4
C,T,0/0,7,2
C,A,0/0,2,2
...
</d-code>

<p>I do realize there are additional filtering criteria such as minium coverage, and base quality scores, but we want to keep this example simple to connect the big ideas together $\text{–}$ and not become lost in the details.</p>

<p>The next step is to prepare these values for training a machine learning model.</p>

<h2 id="defining-the-input-tensor-and-output-labels">Defining the Input (Tensor) and Output Labels</h2>

<p>Now that we have <code class="language-plaintext highlighter-rouge">bcftools</code>-processed candidates from the VCF file, let’s try to generate the input to a neural network.  In our example, the machine learning framework we will use is <a href="https://pytorch.org/" rel="external nofollow noopener" target="_blank">PyTorch</a>.  For a neural network model in PyTorch, the standard input is a <a href="https://pytorch.org/docs/stable/tensors.html" rel="external nofollow noopener" target="_blank">tensor</a>.</p>

<p>Since we’ll be taking a simpler approach, we will base our genotype predictions based on the following ratio (the symbols $||$ just refer to count or cardinality):</p>

\[genotype \; \propto \dfrac{|reference|}{|reference| + |alternate|}\]

<p>Regarding the output genotype labels, they would need to have some form of numerical class label in order to be computable.  Thus let’s label $homozygous \; reference$, $heterozygous$, and $heterozygous \; alternate$ as $1$ through $3$ $\text{–}$ with missing or uncallable (./.) labeled as $0$:</p>

\[./. \rightarrow 0\]

\[0/0 \rightarrow 1\]

\[0/1 \: or \: 1/0 \rightarrow 2\]

\[1/1 \rightarrow 3\]

<p>Thus before going too deep, let’s plot these and visualize what the genotype classes look like:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/2023-11-04-shallowvariant/Linear_Read_Depth.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 1: This illustrates the plot of the fraction of reference read counts versus alternate read counts as compared to the total read depth.
</div>

<p>Based on the figure above, the genotype classes do not seem to separate very well, thus making the stratification by genotype-classes is not as clean.  The fix would be to transform the space the data lives in, and a simple way would be to put them into $log$-$space$ by taking the $log_2$ of the two fractions:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/2023-11-04-shallowvariant/Log2_Read_Depth.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 2: This illustrates the $log_2$ transformed plot of the fraction of reference read counts versus alternate read counts as compared to the total read depth.
</div>

<p>Now there seem to be better separation among the classes, allowing for the weights of the neural network model to be able to classify among the genotypes.</p>

<h2 id="loading-and-processing-the-data">Loading and Processing the Data</h2>

<p>Now we will be getting into the details of how to use PyTorch.  We have a file called <code class="language-plaintext highlighter-rouge">ref_alt_gt_ad.txt</code> that we want to load.  Let’s first start with a few variable definitions and helper functions.</p>

<d-code block="" language="python">
# The reference and alternate allele count fractions
number_of_inputs = 2

# This is the number of neurons in the hidden layer between the input and output
number_of_neurons_in_hidden_layer = 6

# This will represent four classes of predicted outputs: ./., 0/0, (0/1 or 1/0), 1/1
number_of_genotype_classes = 4 

# This represents the number of training or testing rounds (epochs)
number_of_training_or_testing_rounds = 10

# The path of our truth set, containing reference count, allele count and genotypes
labeled_data_file = 'ref_alt_gt_ad.txt'

# Helper function for converting genotypes to numeric labels
def convert_genotype_to_number( genotype ):
    if '0/0' in genotype:
        return 1
    if '0/1' in genotype:
        return 2
    if '1/0' in genotype:
        return 2
    if '1/1' in genotype:
        return 3
    else:
        return 0


# A helper function for converting numeric labels to genotypes 
def convert_numeric_class_to_genotype( numeric_genotype_tensor ):

    genotype_numeric_list = numeric_genotype_tensor.tolist()
    
    genotypes=[]
    
    for i in range(0, len( genotype_numeric_list )):
        if genotype_numeric_list[i] == 0:
            genotypes.append( './.' )
            continue
        if genotype_numeric_list[i] == 1:
            genotypes.append( '0/0' )
            continue
        if genotype_numeric_list[i] == 2:
            genotypes.append( '0/1' )
            continue
        if genotype_numeric_list[i] == 3:
            genotypes.append( '1/1' )
            continue
    
    return genotypes


# Helper function for determining the number of rows in a file
def get_total_rows( data_file ):
    with open(data_file, newline='') as candidates_file:
        data = csv.reader(candidates_file, delimiter=' ', quotechar='|')
        row_counter = 0
        for row in data:
            row_counter = row_counter + 1
        return row_counter

# Defining the number of rows in a file for array instantiation
number_of_rows = int( get_total_rows( labeled_data_file ) )

# Defining NumPy arrays that will contain the reference and allele fractions
X = np.zeros( (number_of_rows, number_of_inputs), dtype=float )

# Defining a NumPy array for output (genotype) labels 
y = np.zeros( (number_of_rows), dtype=int )
</d-code>

<p>The <code class="language-plaintext highlighter-rouge">X</code> and <code class="language-plaintext highlighter-rouge">y</code> vectors are instantiated as <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html" rel="external nofollow noopener" target="_blank">NumPy arrays</a> $\text{–}$ with all zero values $\text{–}$ and having the following dimensions (shapes):</p>

<ul>
  <li>X has dimensions 74732 rows x 2 columns, where two represents the reference and alternate allele</li>
  <li>y has dimensions 74732 elements to store the genotypes.  You can think of it having one
74732 rows x 1 column, which is equivalent.</li>
</ul>

<p>The values in X look as follows:</p>
<d-code block="" language="python">
print(X)

array([[0., 0.],
       [0., 0.],
       [0., 0.],
       ...,
       [0., 0.],
       [0., 0.],
       [0., 0.]])

print( X.shape )

(74732, 2)

# This shows the type being a NumPy ndarray class
print( type(X) )

&lt;class 'numpy.ndarray'&gt;
</d-code>

<p>The values in y look as follows:</p>

<d-code block="" language="python">
print( y )

array([0, 0, 0, ..., 0, 0, 0])

print( y.shape )

(74732,)

# This shows the type being a NumPy ndarray class
print( type(y) )

&lt;class 'numpy.ndarray'&gt;
</d-code>

<p>Next let’s define the function for loading the data:</p>

<d-code block="" language="python">
def load_data( filename=labeled_data_file ):
    with open(filename, newline='') as candidates_file:
        data = csv.reader(candidates_file, delimiter=' ', quotechar='|')
        row_counter = 0
        for row in data:
            row_element = row[0]
            row_split = row_element.split(',')
            if len(row_split) &gt; 5: # skip multi-allelic sites
                continue
            reference = row_split[0]
            alternate = row_split[1]
            genotype = row_split[2]
            reference_read_count = int( row_split[3] )
            alternate_read_count = int( row_split[4] )

            total_read_count = reference_read_count + alternate_read_count
            
            # Skip low coverage sites
            if total_read_count &lt; 10:
                continue
            
            # In case of zero reference counts (for division by zero errors).
            # An increment of 1 will not impact the overall classification results,
            # because the minimum read counts would need to be at least 10.
            if reference_read_count == 0:
                reference_read_count = 1
                total_read_count = total_read_count + 1
                
            # In case of zero alternate counts (for division by zero errors).
            # An increment of 1 will not impact the overall classification results,
            # because the minimum read counts would need to be at least 10.
            if alternate_read_count == 0:
                alternate_read_count = 1
                total_read_count = total_read_count + 1
                        
            reference_reads_to_fill = int( number_of_inputs * 
                                           fraction( reference_read_count, 
                                                     total_read_count ) )
            
            # Log2 Transformed Read Ratios
            # For reference-supporting read counts
            X[ row_counter ][0] = math.log( float(reference_read_count) / 
                                            float(total_read_count), 2 )

            # For alternate allele-supporting read counts
            X[ row_counter ][1] = math.log( float(alternate_read_count) / 
                                            float(total_read_count), 2 )
            
            # Convert the genotypes to numeric labels
            y[ row_counter ] = convert_genotype_to_number( genotype )
            
            row_counter = row_counter + 1

load_data( filename=labeled_data_file )
</d-code>

<p>Next we will define the multi-class neural network model.</p>

<h2 id="defining-the-multiclass-neural-network-model">Defining the MultiClass Neural Network Model</h2>

<p>Let’s define the general network model for classifying for multiple genotype labels (i.e. more than two, or binary):</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/2023-11-04-shallowvariant/multiclass_model.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 3: This illustrates the multiclass neural network model we will be implementing.
</div>

<p>This is the same 2-layer neural network model you have seen before, but there are a few new additions.  One of the most obvious additions is the increased number of nodes.  We put more nodes at the beginning to get more details and the summarized the pertinent information by selecting only four nodes in the second hidden layer.  That is a very common approach in neural network design to have more nodes initially and then focusing the results down the network to fewer and fewer ones.</p>

<p>Another element in this network is the new activation function called $ReLU$, which stands for <em>Rectified Linear Unit</em>.  Its behavior is by returning a value of $0$ for any input below 0, and returning back the input for anything higher than $0$.  This is defined as follows:</p>

\[f(x) = max(0, x) =
    \begin{cases}
      0 &amp; f(x) &lt; 0 \\
      x &amp; f(x) \geq 0
    \end{cases}\]

<p>Here x can represent complex interactions among multiple variables, which is transmitted to the next nodes of the model.  The graph below shows the general behavior of the activation function:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/2023-11-04-shallowvariant/relu_activation_function.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 4: This is the graphical representation of the ReLU activation function.
</div>

<p>The other characteristic that this model contains is the $softmax$ function, which turns a vector of values into probabilities (i.e. the sum of the values will then be equal to 1).  The $softmax$ function is the following:</p>

\[\sigma(\vec{z}) = \dfrac{e^{z_i}}{\Sigma^{K}_{j=1}e^{z_j}}\]

<p>Let’s begin to code the model up:</p>

<d-code block="" language="python">
class GenotypeClassifier(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.main = torch.nn.Sequential(
            torch.nn.Linear(input_size, hidden_size),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_size, output_size)
        )

    def forward(self, x):
        out = self.main(x)
        return out
</d-code>

<p>You will notice that we inherit the class <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html" rel="external nofollow noopener" target="_blank">torch.nn.Module</a>, which is the base class for neural networks in PyTorch.  Then we utilize the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" rel="external nofollow noopener" target="_blank">torch.nn.Linear</a> class, which performs the linear transformations of $X \cdot W^T + bias$ calculations (where $W^T$ is the transpose of the weights).</p>

<p>One more important element when building a model is to randomize the data, and then use 80% for training the model and 20% for subsequently testing it.  The testing step usually is not randomized.  In our case we will randomly shuffle both stages, as the data loading methods in PyTorch will keep providing the same input for every round, which would not perform proper validation.</p>

<p>Let’s now train and test the neural network.</p>

<h2 id="implementing-the-multiclass-neural-network-model">Implementing the MultiClass Neural Network Model</h2>

<p>The first step is to define the model and a few initial variables that we will use:</p>

<d-code block="" language="python">
# We will need to split our data into a training and testing group.
# The training set will be composed of randomly selected values 
# representing 80% of the data.
# The testing set will be composed of randomly selected values 
# representing 20% of the data.
TEST_SIZE = 0.2

# The batch size represents how much of the data at a time will 
# be used during rounds of training or testing (epochs).  In this 
# case an epoch will represent a round of training, with a total 
# of 10 rounds.  In each round, 6700 values of the input will 
# be used.
BATCH_SIZE = 6700

# This is a value to randomize the random number generator used 
# when selecting values.
SEED = 42

# Here we will generate the training set, and the test set for both 
# the input (X) and output (y).  Here the input represents the ratios 
# of reference and alternate allele counts compared to the total.  
# The output represents the numeric genotype values.
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=TEST_SIZE,
    random_state=SEED
)

# Here we store the size of the training and test sets for calculating the accuracy of
# our model's predictions.
train_size = len(X_train)
test_size = len(X_test)
</d-code>

<p>If we print the values for <code class="language-plaintext highlighter-rouge">train_size</code> and <code class="language-plaintext highlighter-rouge">test_size</code> they will be 59785 and 14947, respectively.</p>

<p>The next we will generate the <a href="https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html" rel="external nofollow noopener" target="_blank">tensors</a>.  A tensor is the equivalent of the NumPy array, and is the standard data format in machine learning platforms, since it can remember additional parameters such as if it should reside on a GPU or CPU.  Usually the shape (dimensions) of a tensor is defined as $(batch, channels, rows, columns)$, where channels are different data values of the rows and column, such as RGB (red, green, blue) values for an image.  Batches are how many groups (sets) of the dimensions below $(channels, rows, columns)$ should be taken together in a calculation, such as for training or testing (validation).</p>

<p>Below are the instantiations of the tensors:</p>

<d-code block="" language="python">
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.int64)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.int64)
</d-code>

<p>Next we will create <a href="https://pytorch.org/docs/stable/data.html#iterable-style-datasets" rel="external nofollow noopener" target="_blank">dataset tensors</a> to perform on-the-fly (lazy) data set reprentations for both training and testing.  We will use <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="external nofollow noopener" target="_blank">dataloaders</a> to shuffle and iterate over them.  This <a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html" rel="external nofollow noopener" target="_blank">nice tutorial</a> provides a more friendly definition with examples.</p>

<d-code block="" language="python">
dataset_train = TensorDataset(X_train, y_train)
dataset_test = TensorDataset(X_test, y_test)

dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)
dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)
</d-code>

<p>Finally, let us define our model:</p>

<d-code block="" language="python">
# Here we initialize the GenotypeClassifier with:
#  * The number of inputs
#  * The number of nodes in the hidden layer
#  * The number of outputs
# The instantiation below is equivalent to: 
#    shallow_variant_model = GenotypeClassifier(2, 6, 4)
shallow_variant_model = GenotypeClassifier( number_of_inputs, 
                                            number_of_neurons_in_hidden_layer, 
                                            number_of_genotype_classes )
</d-code>

<p>We will be training the model in batches, since we do not have enough computing power to train the whole dataset at once.  Therefore with each training round (epoch), we take a batch and compare after training with how it performs against the expected (true) <code class="language-plaintext highlighter-rouge">y</code> values.  Basically we compare the predicted $y$ with the true $y$, and based on their difference we optimize the model.  With a bigger difference we optimize more, and vice versa if less.  The $criterion$ measures how wrong our model performs, which is usually called the $loss \; (cost) \; function$.  The loss will be used by the $optimizer \; function$ to adjust the model’s weights for the next round.  Since we will be classifying across multiple classes, we will utilize the <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" rel="external nofollow noopener" target="_blank">CrossEntropyLoss</a> function:</p>

<d-code block="" language="python">
# The Loss function
criterion = torch.nn.CrossEntropyLoss() 
</d-code>

<p>The basic formula for cross entropy loss is the following:</p>

\[Loss(\hat{y}, y) = - \Sigma^{K}_{k} y_k \cdot log( \hat{y}_k )\]

<p>Assuming the values of y are between 0 and 1, what the above formula calculates is the deviation among predicted outputs $\hat{y}$ in a batch, as compared to true (expected) values ($y$).  What it really performs is the <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="external nofollow noopener" target="_blank">information (entropy, $H$)</a> when having varied distributions.  If the values are random, then these values will be high, and thus have a big loss (wrong result), requiring a larger shift in the weights of the neural network.  Otherwise, they will stabilize and the loss will become minimal $\text{–}$ which is ideal $\text{–}$ making the model highly accurate in predicting the outcome.</p>

<p>Given the $loss \; function$ (or $criterion$), there needs to be another function to optimize based on the loss.  For that we will use the <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html" rel="external nofollow noopener" target="_blank">Adam optimizer</a>:</p>

<d-code block="" language="python">
# The (Adam) optimization algorithm
optimizer = torch.optim.Adam( shallow_variant_model.parameters(), lr=0.2 )  
</d-code>

<p>The Adam (Adaptive Moment Estimation) optimizer<d-cite key="kingma2017adam"></d-cite> was first proposed in 2014 (with multiple updates following it) by Diederik P. Kingma and Jimmy Ba.  The general intuition is that it has the advantage over other ones by adaptively adjusting the learning rate of each weight based on previous gradients, thus it converges much faster.  The <code class="language-plaintext highlighter-rouge">lr</code> parameter is the initial learning rate to start with, meaning by how much to adjust the weights the first time when no gradients are available.</p>

<p>Now we are ready to begin to train the model.</p>

<h2 id="training-the-model">Training the Model</h2>

<p>Now we are ready to run through 10 epochs (rounds) to train the model, while checking on its loss and accuracy:</p>

<d-code block="" language="python">
for epoch in range( number_of_training_or_testing_rounds ):
    losses = 0
    correct = 0
    total = 0
    for X_batch, y_batch in dataloader_train:
        optimizer.zero_grad()                    # Reset the gradients for re-optimization
        y_pred = shallow_variant_model(X_batch)  # Run the model to get the predictions
        loss = criterion(y_pred, y_batch)        # Calculate the loss
        loss.backward()                          # Determine the gradients of the weights and biases
        optimizer.step()                         # Update the weights biases
        losses = losses + loss.item()            # Add the loss for this batch to the running total
        correct = correct + correct_count( y_pred, y_batch )
        total = total + 1
    print( f"epoch: {epoch + 1} | loss: {losses / len(dataloader_train):.4f} | 
             accuracy: {100*(correct / train_size):.2f}" )             
</d-code>

<p>You might notice that we have a <code class="language-plaintext highlighter-rouge">correct_count()</code> function specified above.  All that function performs is to determine how many of the predicted values <code class="language-plaintext highlighter-rouge">y_pred</code> match with the true values of <code class="language-plaintext highlighter-rouge">y_batch</code>.  The contents of the function are the following:</p>

<d-code block="" language="python">
def correct_count( y_pred, y_true ):
    correct = 0
    y_pred_argmax = torch.argmax(y_pred, dim=1)
     
    for i in range(0, len(y_pred_argmax) ):
        if (y_pred_argmax[i] - y_true[i]) == 0:
            correct = correct + 1
    
    return correct
</d-code>

<p>While training the model, below is the output of each epoch:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Epoch</th>
      <th style="text-align: center">Loss</th>
      <th style="text-align: center">Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.2682</td>
      <td style="text-align: center">97.07 %</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.0249</td>
      <td style="text-align: center">98.19 %</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.0203</td>
      <td style="text-align: center">99.26 %</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.0179</td>
      <td style="text-align: center">99.26 %</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.0141</td>
      <td style="text-align: center">99.30 %</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: center">0.0137</td>
      <td style="text-align: center">99.36 %</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: center">0.0124</td>
      <td style="text-align: center">99.39 %</td>
    </tr>
    <tr>
      <td style="text-align: center">8</td>
      <td style="text-align: center">0.0121</td>
      <td style="text-align: center">99.42 %</td>
    </tr>
    <tr>
      <td style="text-align: center">9</td>
      <td style="text-align: center">0.0119</td>
      <td style="text-align: center">99.41 %</td>
    </tr>
    <tr>
      <td style="text-align: center">10</td>
      <td style="text-align: center">0.0117</td>
      <td style="text-align: center">99.45 %</td>
    </tr>
  </tbody>
</table>

<p>Below is a graph representing the above results:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/2023-11-04-shallowvariant/training_loss_and_accuracy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 5: This shows the relationship of the loss and accuracy of the model, while it is being trained throughout the epochs.
</div>

<p>The general idea is that a model as it is being trained with each round will have a lower loss and become more accurate, which this is being exhibited here.</p>

<h2 id="testing-the-model">Testing the Model</h2>

<p>Now we are ready to test how good our model behaves with testing data, which it has not seen before.  Again we will go through 10 epochs (rounds) of testing, while checking on its loss and accuracy:</p>

<d-code block="" language="python">
shallow_variant_model.eval()
with torch.inference_mode():
    for epoch in range( number_of_training_or_testing_rounds ):
        losses = 0
        correct = 0
        total = 0
        for X_batch, y_batch in dataloader_test:
            y_pred = shallow_variant_model(X_batch)  # Run the model to get the predictions
            loss = criterion(y_pred, y_batch)        # Calculate the loss
            losses = losses + loss.item()            # Add the loss for this batch to the running total
            correct = correct + correct_count( y_pred, y_batch )
            total = total + 1
        print( f"epoch: {epoch + 1} | loss: {losses / len(dataloader_test):.4f} | 
                 accuracy: {100*(correct / test_size):.2f}")                 
</d-code>

<p>While testing the model, below is the output of each epoch:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Epoch</th>
      <th style="text-align: center">Loss</th>
      <th style="text-align: center">Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.0095</td>
      <td style="text-align: center">99.61 %</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.0105</td>
      <td style="text-align: center">99.61 %</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.0110</td>
      <td style="text-align: center">99.61 %</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.0093</td>
      <td style="text-align: center">99.61 %</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.0119</td>
      <td style="text-align: center">99.61 %</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: center">0.0106</td>
      <td style="text-align: center">99.61 %</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: center">0.0108</td>
      <td style="text-align: center">99.61 %</td>
    </tr>
    <tr>
      <td style="text-align: center">8</td>
      <td style="text-align: center">0.0096</td>
      <td style="text-align: center">99.61 %</td>
    </tr>
    <tr>
      <td style="text-align: center">9</td>
      <td style="text-align: center">0.0094</td>
      <td style="text-align: center">99.61 %</td>
    </tr>
    <tr>
      <td style="text-align: center">10</td>
      <td style="text-align: center">0.0106</td>
      <td style="text-align: center">99.61 %</td>
    </tr>
  </tbody>
</table>

<p>Below is a graph reprenting the above results:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/2023-11-04-shallowvariant/testing_loss_and_accuracy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 6: This shows the relationship of the loss and accuracy, while the model is being tested throughout the epochs.
</div>

<p>Based on the low loss and high accuracy the model performs fairly well.</p>

<h2 id="making-some-predictions">Making Some Predictions</h2>

<p>Now let us try to predict some genotypes.  We will take the following sets of read counts for different positions:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Position</th>
      <th style="text-align: center">Reference</th>
      <th style="text-align: center">Alternate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">10</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">10</td>
      <td style="text-align: center">10</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">10</td>
    </tr>
  </tbody>
</table>

<p>We will need to take their fractions based on the total number of reads at each position, and then take the $log_2$ of each of those fractions:</p>

<d-code block="" language="python">
# The reads are of the form: [reference, alternate]
# We will divide them by their total and take the log2 of each,
# before we submit them to the model
predition_reads_fractions = torch.log2(
  torch.div( 
    torch.tensor([[10,1], [10,10], [1,10]], dtype=torch.float32 ), 
    torch.tensor([[10+1], [10+10], [1+10]], dtype=torch.float32 )
  )
)

# The unprocessed predictions from the model
prediction = shallow_variant_model( predition_reads_fractions ).detach()
print( "The unprocessed predictions from the model: \n" )
print( prediction )

# The softmax transformations of the predictions to probabilities
probability = nn.Softmax( dim=1 )( prediction )
print( "\nThe softmax transformations of the predictions to probabilities: \n" )
print( probability )

# Checking that the probabilities add up to 1
print( "\nChecking that the probabilities add up to 1: \n" )
print( probability.sum( dim=1 ) )

# The numeric genotype predictions
print( "\nThe probability after argmax: \n" )
classes = probability.argmax( dim=1 )
print( classes )

# The genotype of the numerically-transformed predictions
print( "\nThe genotype of the numerically-transformed predictions: \n" )
classes = probability.argmax( dim=1 )
print( convert_numeric_class_to_genotype( classes ) )

# The parameters of the trained model
print( "\nThe parameters of the trained model:\n" )
print( shallow_variant_model.state_dict() )
</d-code>

<p>The output result of the predictions is the following:</p>

<d-code block="" language="python">
The unprocessed predictions from the model: 

tensor([[-40.8362,  19.1928,  16.7496,  -6.0768],
        [-20.0807,   0.3643,   6.1820,  -5.4118],
        [-35.6244, -17.7323,   4.2061,   7.6862]])

The softmax transformations predictions to probabilities: 

tensor([[7.8263e-27, 9.2006e-01, 7.9939e-02, 9.7584e-12],
        [3.9171e-12, 2.9654e-03, 9.9703e-01, 9.1953e-06],
        [1.5040e-19, 8.8650e-12, 2.9883e-02, 9.7012e-01]])

Checking that the probabilities add up to 1: 

tensor([1.0000, 1.0000, 1.0000])

The numeric genotype predictions: 

tensor([1, 2, 3])

The genotype of the numerically-transformed predictions: 

['0/0', '0/1', '1/1']
</d-code>

<p>The results seem promising and will always get better with more data and more rounds of training.</p>

<p>One of the reasons I started this blog series is to unmystify what the models actually are performing internally.  In the next section we will dive a bit deeper on what the model actually learned from the read depths to predict the genotype.</p>

<h2 id="what-has-the-model-learned-diving-deeper-into-the-genotype-model">What has the model learned? Diving Deeper Into the Genotype Model</h2>

<p>PyTorch has made it easy to look inside the model to determine what the weights have learned from the data:</p>

<d-code block="" language="python">

# The parameters of the trained model
print( "\nThe parameters of the trained model:\n" )
print( shallow_variant_model.state_dict() )
</d-code>

<p>The result is the following:</p>

<d-code block="" language="python">
The parameters of the trained model:

OrderedDict([('main.0.weight', tensor([[ 0.9628,  1.8207],
        [ 0.1478,  0.9089],
        [-1.2859, -3.0906],
        [-2.5636,  0.2988],
        [ 1.6635,  1.3060],
        [ 1.4580,  1.6511]])), ('main.0.bias', tensor([-1.6755,  2.9028,  2.2591, -0.6324,  1.6223,  1.8639])), ('main.2.weight', tensor([[-0.9532,  1.7402, -3.2636, -2.2329,  1.6347,  1.9490],
        [-0.8620, -2.1863,  1.5901, -2.7379, -2.1378, -1.5735],
        [-1.4391, -0.5111,  1.3885, -0.3735, -2.3134, -1.8907],
        [ 1.2431, -2.6779, -0.3130,  2.1911, -1.5693, -1.8159]])), ('main.2.bias', tensor([ 2.0075, -1.6815, -1.4783, -1.9677]))])
</d-code>

<p>Mathematically these can be represented as the following:</p>

<h3 id="hidden-layer-i-hl1"><ins>Hidden Layer I (HL1)</ins></h3>

\[\overrightarrow{\textbf{HL1}} = \begin{bmatrix}
 0.9628 &amp;  1.8207 \\
 0.1478 &amp;  0.9089 \\
-1.2859 &amp; -3.0906 \\
-2.5636 &amp;  0.2988 \\
 1.6635 &amp;  1.3060 \\
 1.4580 &amp;  1.6511
\end{bmatrix} 
\cdot
\begin{bmatrix}
log_2( \dfrac{reference}{total} ) \\
log_2( \dfrac{alternate}{total} )
\end{bmatrix} +\]

\[+ \begin{bmatrix}
-1.6755 \; 2.9028 \; 2.2591 \; -0.6324 \; 1.6223 \; 1.8639
\end{bmatrix}\]

<h3 id="hidden-layer-ii-hl2"><ins>Hidden Layer II (HL2)</ins></h3>

\[\overrightarrow{\textbf{HL2}} = \begin{bmatrix}
-0.9532 &amp;  1.7402 &amp; -3.2636 &amp; -2.2329 &amp;  1.6347 &amp;  1.9490 \\
-0.8620 &amp; -2.1863 &amp;  1.5901 &amp; -2.7379 &amp; -2.1378 &amp; -1.5735 \\
-1.4391 &amp; -0.5111 &amp;  1.3885 &amp; -0.3735 &amp; -2.3134 &amp; -1.8907 \\
 1.2431 &amp; -2.6779 &amp; -0.3130 &amp;  2.1911 &amp; -1.5693 &amp; -1.8159
\end{bmatrix} 
\cdot
ReLU( \overrightarrow{\textbf{HL1}} ) +\]

\[+ \begin{bmatrix}
2.0075 &amp; -1.6815 &amp; -1.4783 &amp; -1.9677
\end{bmatrix}\]

<p>Let us now try to inspect the effect of the weights across the range of read fractions.  If we take a look at the outputs of <em>Hidden Layer I</em>, they are in the range from {$0.0001,…,0.9999$}.  The node output distribution (for Hidden Layer I) across that range is as follows:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/2023-11-04-shallowvariant/hiddel_layer_1_outputs.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 7: This shows the change in the outputs of Hidden Layer I for the range of possible inputs of read fractions.
</div>

<p>Another way the analysis can be viewed, is from the perspective of the binary contribution of the weights.  The inputs are $(p, 1-p)$ where $p \in $ {$0,…,1$}, as is $1-p$.  Thus given the log plot of the values, the highest contribution would be around $0.5$:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/2023-11-04-shallowvariant/log_p_vs_1_p.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 8: This shows the contributions of the weights among the two inputs based on the log-transformed values.
</div>

<p>Thus the <em>ReLU</em> activation function will remove any negative contributions for upcoming layers of network.  With that knowledge in mind, let’s explore which nodes actually have the most effect on predicting each genotype class:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/2023-11-04-shallowvariant/relu_activation_outputs.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 9: This shows the change in the outputs of the ReLU activation function, for the range of possible inputs of read fractions.
</div>

<p>Given the above graph, it seems that only the weights of nodes 2, 3, and 4 of Hidden Layer I actually will have significant inpact in the separation of genotypes.  With that knowledge, let’s explore the output of Hidden Layer II:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/2023-11-04-shallowvariant/hiddel_layer_2_outputs.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 10: This shows the change in the outputs of Hidden Layer II for the range of possible inputs of read fractions.
</div>

<p>Based on the separation of genotype classes, only classes $1$ $(0/0)$, $2$ $(0/1)$ and $3$ $(1/1)$ perform separation among each other.  Thus we can use class $1$ $(0/0)$ as a baseline to plot the other two against it, in order to determine separation of genotypes:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/2023-11-04-shallowvariant/point_of_genotype_separation_among_classes.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 11: This shows the separation among genotype classes $2$ $(0/1)$ and $3$ $(1/1)$ with respect to class $1$ $(0/0)$.
</div>

<p>There is a clear point of intersection at around the value of $-14$ of class $1$ $(0/0)$, where a switch takes place among classes $2$ $(0/1)$ and $3$ $(1/1)$ $\text{–}$ emphasized by the weights.</p>

<p>There is more analysis in the works, with which this report will be updated, but I hope I gave you a overview of what neural network models learn and how genotype can be predicted just from the read depth.</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2023-11-04-shallowvariant.bib"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Paul  Grosu. • E-Mail <a href="mail:pgrosu@gmail.com" target="_blank">pgrosu@gmail.com</a> • LinkedIn: <a href="https://www.linkedin.com/in/paul-grosu-6262807/" target="_blank" rel="external nofollow noopener">https://www.linkedin.com/in/paul-grosu-6262807/</a>

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="/assets/js/bootstrap.bundle.min.js"></script>
  <!-- <script src="/assets/js/mdb.min.js"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
